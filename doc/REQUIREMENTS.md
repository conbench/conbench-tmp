# Conbench: Continuous Benchmarking (CB) Framework Requirements

## Context

Continuous Integration (CI) and Continuous Delivery (CD) tools have become
essential to modern software development workflows. These apply systematic
rigor to functional testing, code coverage, packaging, and deployment
throughout the lifecycle of a project.

Tools for "continuous benchmarking" (CB) or performance testing are not nearly
so mature. This has numerous negative consequences:

* Developers use benchmarks infrequently to determine whether to accept
  patches. Application performance regressions are often identified long after
  the offending patch or patches were merged, such as when users install a
  newly released package and find that some production process grew slower or
  more resource intensive.
* The performance of code is seldom tracked over time.
* The impact of dependency upgrades or operating system changes (e.g. Linux
  kernel upgrades) often go unnoticed.
* Developers are less motivated to write "defensive" benchmarks to prevent
  their performance optimization effort from being "undone" by performance
  regressions introduced by subsequent work.

As such, we propose a general purpose, language-agnostic "continuous
benchmarking" framework that can collect language-dependent benchmark data and
operationalize that data to more naturally incorporate performance data into
the software development workflow.

## Non-Goals

Before getting into features and other requirements, this section lists some
things that this project is **not** going to do, at least at this stage.

* Replace language-specific benchmark libraries for **authoring**
  benchmarks. This includes projects like [Google Benchmark][1] for C/C++,
  [JMH][2] for the JVM, [criterion][3] for Rust, etc.

## Features and Other Requirements

Some areas of development for this framework include the following areas:

* Result database
* Benchmark result collection
* Hardware information collection
* Software configuration collection
* Build Configuration and Automation
* Tooling for patch validation
* API for remote reporting
* Front end for exploration and visualization

### Result database

This project requires a database to store and query the data produced by
executing benchmarks. Some considerations for the database:

* Results must be associated with a deterministic revision of a codebase (for
  example, a git hash, but we do not need to only work with git)
* Data may be generated by different machine environments, operating systems,
  and other software configurations.
* A particular machine may generate multiple sets of results for different
  build configurations. For example, using different compilers (like clang
  versus gcc) or different compiler flags (like with or without SIMD compiler
  extensions).
* Data may be commingled from different underlying benchmark frameworks

Users of the CB framework may wish to examine benchmark results across many
different axes of comparison:

* Revisions of the codebase (changes through time)
* Compilers or compiler flags on the same machine
* Dependencies versions (e.g. OpenBLAS vs. Intel MKL)
* Machine / hardware configuration (e.g. CPU type or architecture)

Note that results collected from different machines may be stored in the same
database.

### Benchmark result collection

We assume that developers will continue to author benchmarks using the
preferred microbenchmark or macrobenchmark support libraries for their
programming language.

For each kind of benchmarking library, we propose to write a "plugin" to enable
this project to

* Execute the benchmarks
* Extract and store the results

Some things to keep in mind:

* Not all benchmarks are time-based. Other metrics include throughput
  (e.g. bytes-per-second) or memory use. We should probably retain the
  flexibility to track new kinds of metrics in the future.
* A single benchmark run may produce more than one kind of result measurement,
  such as both time and peak memory use.
* For each kind of measurement, we should know whether "higher" or "lower" is
  better.

One question to be explored is benchmarks that output results from multiple
runs rather than an average or median result. The benchmark may return all of
the results and leave the statistics to us. We should probably have the
flexibility to store many results for the same metric on the same benchmark run
so that we can compute statistics like mean, median, and standard deviation
ourselves.

### Hardware information collection

When executing the benchmarks on a machine, we should collect as detailed
information as possible about the machine and store this in a well-structured
schema:

* CPU identifier and characteristics (manufacturer, architecture, clock speed,
  L1/L2/L3 cache sizes, and so forth)
* Installed RAM
* GPU information (including multi-GPU configurations)

### Software configuration collection

A benchmark run may expose dependency version numbers and other configuration
information. This includes such things as:

* The operating system type and version (this includes the Linux kernel
  version). This is important as operating system updates in recent times have
  included CPU security vulnerability mitigations that affect performance.
* Compiler type and version. For example, a machine may report both gcc 4.8 and
  gcc 8 results
* Installed dependency versions (both build-time and run-time dependencies)

Some of this data collection will have to be provided by common code (for
example, operating system level data) while other data will be provided by
programming language-specific plugins (e.g. reporting installed Python packages
or C++ library toolchain dependencies)

### Build Configuration and Automation

### Tooling for patch validation

### API for remote reporting

### Front end for exploration and visualization

## Implementation Considerations

[1]: https://github.com/google/benchmark
[2]: https://openjdk.java.net/projects/code-tools/jmh/
[3]: https://docs.rs/crate/criterion