# Conbench: Continuous Benchmarking (CB) Framework Requirements

## Context

Continuous Integration (CI) and Continuous Delivery (CD) tools have become
essential to modern software development workflows. These apply systematic
rigor to functional testing, code coverage, packaging, and deployment
throughout the lifecycle of a project.

Tools for "continuous benchmarking" (CB) or performance testing are not nearly
so mature. This has numerous negative consequences:

* Developers use benchmarks infrequently to determine whether to accept
  patches.
* Application performance regressions are often identified long after the
  offending patch or patches were merged, such as when users install a newly
  released package and find that some production process grew slower or more
  resource intensive.
* The performance metrics of a codebase are seldom tracked over time.
* The impact of dependency upgrades or operating system changes (e.g. Linux
  kernel upgrades) often go unnoticed.
* Developers are less motivated to write "defensive" benchmarks to prevent
  their performance optimization effort from being "undone" by performance
  regressions introduced by subsequent work.

To address these issues, we propose a general purpose, language-agnostic
"continuous benchmarking" framework that can collect language-dependent
benchmark data and operationalize that data to more naturally incorporate
performance data into the software development workflow.

## Non-Goals

Before getting into features and other requirements, this section lists some
things that this project is **not** going to do, at least at this initial
stage.

* Replace language-specific benchmark libraries for **authoring**
  benchmarks. This includes projects like [Google Benchmark][1] for C/C++,
  [JMH][2] for the JVM, [criterion][3] for Rust, etc.

## Features and Other Requirements

Some areas of development for this framework include the following areas:

* Database for result storage and associated metadata
* Benchmark result collection
* Hardware information collection
* Software configuration collection
* Build Configuration and Automation
* Tooling for patch validation
* API for remote reporting
* Front end for exploration and visualization

### Result database

This project requires a database to store and query the data produced by
executing benchmarks. Some considerations for the database:

* Results must be associated with a deterministic revision of a codebase (for
  example, a git hash, but we do not need to only work with git). If possible,
  the point in calendar time associated with the code revision should be
  collected.
* Data may be generated by different machine environments, operating systems,
  and other software configurations.
* A particular machine may generate multiple sets of results for different
  build configurations. For example, using different compilers (like clang
  versus gcc) or different compiler flags (like with or without SIMD compiler
  extensions).
* Data may be commingled from different underlying benchmark frameworks

Users of **Conbench** may wish to examine benchmark results across many
different axes of comparison:

* Revisions of the codebase (changes through time)
* Compilers or compiler flags on the same machine
* Dependencies versions (e.g. OpenBLAS vs. Intel MKL)
* Machine / hardware configuration (e.g. CPU type or architecture)

Note that results collected from different machines may be stored in the same
database.

### Benchmark result collection

We assume that developers will continue to author benchmarks using the
preferred microbenchmark or macrobenchmark support libraries for their
programming language.

For each kind of benchmarking library, we propose to write a "plugin" to enable
**Conbench** to

* Execute the benchmarks
* Extract and store the results

Some things to keep in mind:

* Not all benchmarks are time-based. Other metrics include throughput
  (e.g. bytes-per-second) or memory use. We should probably retain the
  flexibility to track new kinds of metrics in the future.
* A single benchmark run may produce more than one kind of result measurement,
  such as both time and peak memory use.
* A benchmark may emit multiple results from a single run rather than the
  average or median results. This is important to measure variability across
  runs as well as differences between the first execution and subsequent
  executions (e.g. caching or other effects). Thus we should be able to store
  multiple measurements for the same metric as the output of a single benchmark
  run.
* For each kind of measurement, we should know whether "higher" or "lower" is
  better.

### Hardware information collection

Many benchmarking configurations will involve a single machine, while others
may involve multiple machines. In any case, the notion of a "configuration"
should allow the entry of hardware information for more than one machine. In
the storage schema, there should also be a place where additional user-defined
metadata fields relating to the configuration can be stored (ones that may not
be specific to a single machine).

For each physical machine, we should collect as detailed information as
possible about the machine and store this in a well-structured schema:

* CPU identifiers and characteristics (manufacturer, architecture, clock speed,
  L1/L2/L3 cache sizes, and so forth). A machine may have more than one CPU
  socket
* Installed RAM
* GPU information. A machine may have more than one GPU
* Any other characteristics of interest (the above details are not exclusive to
  others)

### Software configuration collection

A benchmark run may expose dependency version numbers and other configuration
information. This includes such things as:

* The operating system type and version (this includes the Linux kernel
  version). This is important as operating system updates in recent times have
  included CPU security vulnerability mitigations that affect performance.
* Compiler type and version. For example, a machine may report both gcc 4.8 and
  gcc 8 results
* Installed dependency versions (both build-time and run-time dependencies)

Some of this data collection will have to be provided by common code (for
example, operating system level data) while other data will be provided by
programming language-specific plugins (e.g. reporting installed Python packages
or C++ library toolchain dependencies)

### Build Configuration and Automation

**Conbench** users must do an initial setup / configuration to instruct
**Conbench** how to do a number of things:

* The type of code repository and its location.
* The parameters for each benchmark run to be performed for a revision of the
  codebase. Such parameters may include: environment variables
  (e.g. `CXX=clang-9`), build system configuration options, compiler flags,
  library dependency versions, and other details.
* The template script to invoke to build the project and any benchmark
  executables given a set of configuration parameters
* If desired by the user, the framework could optionally take responsibility
  for cloning and checking out code revisions as part of the benchmarking
  process. Other users may wish to manage the state of their codebase(s) and
  the build process themselves and use **Conbench** only to collect and store
  the data produced by the benchmark runs at a user-supplied revision
  identifier.

Some kind of human-readable YAML-based configuration is likely needed.

Templatized build scripts could use Jinja2 syntax or some other well-known
template syntax for parameters that vary across different benchmark runs.

### Tooling for patch validation

It would be useful to have an easy-to-use command line interface (CLI) tool to
assist developers with comparing the performance of a set of benchmarks between
two commits, such as the master branch versus a feature branch. Such feedback
could be created by automated processes to give feedback on GitHub pull
requests to validate that a patch does not cause performance regressions.

As special cases, this CLI tool should also support comparing local changes
versus a branch (e.g. master), or checking the performance after one or more
branches are merged with master.

### API for remote reporting

Some users may wish to store benchmark results locally, whereas others may wish
to report results from "satellite" machines into a central result server. It
would make sense, therefore, to have a simple REST-type API server
implementation that is able to receive results and write them to its local
database.

Another possibility for an API for remote reporting is to write results to a
remote SQL server (e.g. PostgreSQL). For now, this support is marked "to be
determined".

### Front end for exploration and visualization

Given a result database, users will wish to explore and visualize the results
in some kind of UI, probably in a web browser. A static site would be the
simplest to deploy.

For example, the [Airspeed Velocity project][5] for continuous benchmarking in
Python is able to [generate a static site for browsing results][4] and may
provide some inspiration.

## Implementation Considerations

Python seems like the probable primary implementation language of
**Conbench**. There are a variety of other technical decisions to be made based
on the above and we can accumulate these here.

[1]: https://github.com/google/benchmark
[2]: https://openjdk.java.net/projects/code-tools/jmh/
[3]: https://docs.rs/crate/criterion
[4]: https://pv.github.io/numpy-bench/
[5]: https://github.com/airspeed-velocity/asv