# Conbench: Continuous Benchmarking (CB) Framework Requirements

## Context

Continuous Integration (CI) and Continuous Delivery (CD) tools have become
essential to modern software development workflows. These apply systematic
rigor to functional testing, code coverage, packaging, and deployment
throughout the lifecycle of a project.

Tools for "continuous benchmarking" (CB) or performance testing are not nearly
so mature. This has numerous negative consequences:

* Developers use benchmarks infrequently to determine whether to accept
  patches. Application performance regressions are often identified long after
  the offending patch or patches were merged, such as when users install a
  newly released package and find that some production process grew slower or
  more resource intensive.
* The performance of code is seldom tracked over time.
* The impact of dependency upgrades or operating system changes (e.g. Linux
  kernel upgrades) often go unnoticed.
* Developers are less motivated to write "defensive" benchmarks to prevent
  their performance optimization effort from being "undone" by performance
  regressions introduced by subsequent work.

As such, we propose a general purpose, language-agnostic "continuous
benchmarking" framework that can collect language-dependent benchmark data and
operationalize that data to more naturally incorporate performance data into
the software development workflow.

## Non-Goals

Before getting into features and other requirements, this section lists some
things that this project is **not** going to do, at least at this stage.

* Replace language-specific benchmark libraries for **authoring**
  benchmarks. This includes projects like [Google Benchmark][1] for C/C++,
  [JMH][2] for the JVM, [criterion][3] for Rust, etc.

## Features and Other Requirements

Some areas of development for this framework include the following areas:

* Result storage
* Benchmark result collection
* Hardware information collection
* Software configuration collection
* Build automation
* Tooling for patch validation
* Front end for exploration and visualization

### Result storage

This project requires a database to store and query the data produced by
executing benchmarks. Some considerations for the database:

* Results must be associated with a deterministic revision of a codebase (for
  example, a git hash, but we do not need to only work with git)
* Data may be generated by different machine environments, operating systems,
  and other software configurations.
* A particular machine may generate multiple sets of results for different
  build configurations. For example, using different compilers (like clang
  versus gcc) or different compiler flags (like with or without SIMD compiler
  extensions).
* Data may be commingled from different underlying benchmark frameworks

Users of the CB framework may wish to examine benchmark results across many
different axes of comparison:

* Revisions of the codebase (changes through time)
* Compilers or compiler flags on the same machine
* Dependencies versions (e.g. OpenBLAS vs. Intel MKL)
* Machine / hardware configuration (e.g. CPU type or architecture)

### Benchmark result collection

We assume that developers will continue to author benchmarks using the
preferred microbenchmark or macrobenchmark support libraries for their
programming language.

Given an in-use benchmark support library

### Hardware information collection

### Software configuration collection

### Build Automation

### Front end for exploration and visualization

### Tooling for patch validation

## Implementation Considerations

[1]: https://github.com/google/benchmark
[2]: https://openjdk.java.net/projects/code-tools/jmh/
[3]: https://docs.rs/crate/criterion