# Conbench: Continuous Benchmarking (CB) Framework Requirements

## Context

Continuous Integration (CI) and Continuous Delivery (CD) tools have become
essential to modern software development workflows. These apply systematic
rigor to functional testing, code coverage, packaging, and deployment
throughout the lifecycle of a project.

Tools for "continuous benchmarking" (CB) or performance testing are not nearly
so mature. This has numerous negative consequences:

* Developers use benchmarks infrequently to determine whether to accept
  patches. Application performance regressions are often identified long after
  the offending patch or patches were merged, such as when users install a
  newly released package and find that some production process grew slower or
  more resource intensive.
* The performance of code is seldom tracked over time.
* The impact of dependency upgrades or operating system changes (e.g. Linux
  kernel upgrades) often go unnoticed.
* Developers are less motivated to write "defensive" benchmarks to prevent
  their performance optimization effort from being "undone" by performance
  regressions introduced by subsequent work.

As such, we propose a general purpose, language-agnostic "continuous
benchmarking" framework that can collect language-dependent benchmark data and
operationalize that data to more naturally incorporate performance data into
the software development workflow.

## Non-Goals

Before getting into features and other requirements, this section lists some
things that this project is **not** going to do, at least at this stage.

* Replace language-specific benchmark libraries for **authoring**
  benchmarks. This includes projects like [Google Benchmark][1] for C/C++,
  [JMH][2] for the JVM, [criterion][3] for Rust, etc.

## Features and Other Requirements

Some areas of development for this framework include the following areas:

* Result database
* Benchmark result collection
* Hardware information collection
* Software configuration collection
* Build Configuration and Automation
* Tooling for patch validation
* API for remote reporting
* Front end for exploration and visualization

### Result database

This project requires a database to store and query the data produced by
executing benchmarks. Some considerations for the database:

* Results must be associated with a deterministic revision of a codebase (for
  example, a git hash, but we do not need to only work with git)
* Data may be generated by different machine environments, operating systems,
  and other software configurations.
* A particular machine may generate multiple sets of results for different
  build configurations. For example, using different compilers (like clang
  versus gcc) or different compiler flags (like with or without SIMD compiler
  extensions).
* Data may be commingled from different underlying benchmark frameworks

Users of Conbench may wish to examine benchmark results across many
different axes of comparison:

* Revisions of the codebase (changes through time)
* Compilers or compiler flags on the same machine
* Dependencies versions (e.g. OpenBLAS vs. Intel MKL)
* Machine / hardware configuration (e.g. CPU type or architecture)

Note that results collected from different machines may be stored in the same
database.

### Benchmark result collection

We assume that developers will continue to author benchmarks using the
preferred microbenchmark or macrobenchmark support libraries for their
programming language.

For each kind of benchmarking library, we propose to write a "plugin" to enable
this project to

* Execute the benchmarks
* Extract and store the results

Some things to keep in mind:

* Not all benchmarks are time-based. Other metrics include throughput
  (e.g. bytes-per-second) or memory use. We should probably retain the
  flexibility to track new kinds of metrics in the future.
* A single benchmark run may produce more than one kind of result measurement,
  such as both time and peak memory use.
* A benchmark may emit multiple results from a single run rather than the
  average or median results. This is important to measure variability across
  runs as well as differences between the first execution and subsequent
  executions (e.g. caching or other effects). Thus we should be able to store
  multiple measurements for the same metric as the output of a single benchmark
  run.
* For each kind of measurement, we should know whether "higher" or "lower" is
  better.

### Hardware information collection

When executing the benchmarks on a machine, we should collect as detailed
information as possible about the machine and store this in a well-structured
schema:

* CPU identifier and characteristics (manufacturer, architecture, clock speed,
  L1/L2/L3 cache sizes, and so forth)
* Installed RAM
* GPU information (including multi-GPU configurations)

### Software configuration collection

A benchmark run may expose dependency version numbers and other configuration
information. This includes such things as:

* The operating system type and version (this includes the Linux kernel
  version). This is important as operating system updates in recent times have
  included CPU security vulnerability mitigations that affect performance.
* Compiler type and version. For example, a machine may report both gcc 4.8 and
  gcc 8 results
* Installed dependency versions (both build-time and run-time dependencies)

Some of this data collection will have to be provided by common code (for
example, operating system level data) while other data will be provided by
programming language-specific plugins (e.g. reporting installed Python packages
or C++ library toolchain dependencies)

### Build Configuration and Automation

Conbench users must do an initial setup / configuration to instruct Conbench
how to do a number of things:

* The type of code repository and its location. Conbench should probably drive
  the codebase clone and checkout process to avoid user errors and enhance
  reproducibility
* The parameters for each benchmark run to be performed for a revision of the
  codebase. Such parameters may include: environment variables
  (e.g. `CXX=clang-9`), build system configuration options, compiler flags,
  library dependency versions, and other details.
* The template script to invoke to build the project and any benchmark
  executables given a set of configuration parameters

Some kind of human-readable YAML-based configuration is likely called
for. Templatized build scripts could use Jinja2 syntax or some other well-known
template syntax for parameters that vary across different benchmark runs.

### Tooling for patch validation

It would be useful to have an easy-to-use command line interface (CLI) tool to
assist developers with comparing the performance of a set of benchmarks between
two commits, such as the master branch versus a feature branch. Such feedback
could be created by automated processes to give feedback on GitHub pull
requests to validate that a patch does not cause performance regressions.

As special cases, this CLI tool should also support comparing local changes
versus a branch (e.g. master), or checking the performance after one or more
branches are merged with master.

### API for remote reporting

Some users may wish to store benchmark results locally, whereas others may wish
to report results from "satellite" machines into a central result server. It
would make sense, therefore, to have a simple REST-type API server
implementation that is able to receive results and write them to its local
database.

Another possibility for an API for remote reporting is to write results to a
remote SQL server (e.g. PostgreSQL). This will be to determined.

### Front end for exploration and visualization

Given a result database, users will wish to explore and visualize the results
in some kind of UI, probably in a web browser. A static site would be the
simplest to deploy.

For example, the [Airspeed Velocity project][5] for continuous benchmarking in
Python is able to [generate a static site for browsing results][4] and may
provide some inspiration.

## Implementation Considerations

Python seems like the probable primary implementation language of
Conbench. There are a variety of other technical decisions to be made based on
the above and we can accumulate these here.

[1]: https://github.com/google/benchmark
[2]: https://openjdk.java.net/projects/code-tools/jmh/
[3]: https://docs.rs/crate/criterion
[4]: https://pv.github.io/numpy-bench/
[5]: https://github.com/airspeed-velocity/asv